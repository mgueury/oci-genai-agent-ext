#Import the required packages
from oci import config, retry
from oci.generative_ai_inference import GenerativeAiInferenceClient
from oci.generative_ai_inference.models import CohereChatRequest, \
    OnDemandServingMode, ChatDetails, CohereTool, CohereParameterDefinition, CohereToolResult
import oci
from datetime import datetime, timezone, timedelta
import streamlit as st

compartment_id = os.getenv("TF_VAR_compartment_ocid")

signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
monitoring_client = oci.monitoring.MonitoringClient(config={}, signer=signer)

# Service endpoint for frankfurt region.Change the region if needed.
endpoint = "https://inference.generativeai.eu-frankfurt-1.oci.oraclecloud.com"
generative_ai_inference_client = GenerativeAiInferenceClient(config={},signer=signer,
                                                             service_endpoint=endpoint,
                                                             retry_strategy=retry.NoneRetryStrategy(),
                                                             timeout=(10, 240))

# To format the date
def date_formatter(day):
    return day.isoformat('T', 'milliseconds') + 'Z'

#To list alarms in a compartment
def list_alarm(name):
    list_alarms_response = monitoring_client.list_alarms(
        compartment_id=compartment_id,
        display_name=name,
        lifecycle_state="ACTIVE")
    return list_alarms_response.data[0].id

#To check history of alarms in the last two days.Limited to two days to reduce input data
def alarm_history(**kwargs):
    alarm_response = monitoring_client.get_alarm_history(
        alarm_id=list_alarm(kwargs.get('name')),
        alarm_historytype="STATE_HISTORY",
        timestamp_greater_than_or_equal_to=date_formatter(two_days_before),
        timestamp_less_than=date_formatter(now))
    alarm_history_data = []
    for data in alarm_response.data.entries:
        summary = data.summary.removeprefix('The alarm state is ')
        alarm_history_data.append({f"State_{str(data.timestamp_triggered)}": summary})
    return alarm_history_data

#To check alarms in FIRING state in a compartment.
def alarm_status():
    alarm_status_response = monitoring_client.list_alarms_status(
        compartment_id=compartment_id,
        compartment_id_in_subtree=False,
        sort_by="severity",
        sort_order="DESC",
        status="FIRING")
    return alarm_status_response.data


functions_map = {
    "alarm_history": alarm_history,
    "alarm_status": alarm_status
}

#Tool parameter definition
alarm_history_param = CohereParameterDefinition()
alarm_history_param.description = "alarm name"
alarm_history_param.type = "str"
alarm_history_param.is_required = True

#Tool definitions
tool3 = CohereTool()
tool3.name = "alarm_history"
tool3.description = "The tool will help you find information related to specific alarm history in Oracle Cloud within the specified time"
tool3.parameter_definitions = {
    "name": alarm_history_param
}

tool5 = CohereTool()
tool5.name = "alarm_status"
tool5.description = "The tool will help you analyze alarms status across services and give you correlated report. This tool will be called during root cause analysis or RCA as well to give insights on alarm"

#list of tools
tools = [tool3, tool5]

# streamlit
st.title('Cohere Tools with OCI GenAI')

user_input = st.chat_input("Enter your observability query in plain text:")
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []


def reset_conversation():
    st.session_state.chat_history = []


st.button('Reset Chat', on_click=reset_conversation)

if st.session_state.chat_history is not None:
    for chat_history in st.session_state.chat_history:
        with st.chat_message(chat_history["role"]):
            st.markdown(chat_history["message"])

if user_input:
    def ai_response():
        chat_detail = ChatDetails()
        chat_detail.serving_mode = OnDemandServingMode(model_id="cohere.command-r-plus-08-2024")
        chat_detail.compartment_id = compartment_id

        chat_request = CohereChatRequest(chat_history=st.session_state.chat_history, tools=tools)
        chat_request.max_tokens = 4000
        chat_request.temperature = 0
        chat_request.frequency_penalty = 1
        chat_request.top_p = 0.75
        chat_request.api_format = "COHERE"
        chat_request.is_stream = False
        chat_request.preamble_override = "When a question is irrelevant or unrelated to the available tools, please choose to directly answer it"

        chat_request.message = user_input

        chat_detail.chat_request = chat_request
        chat_response = generative_ai_inference_client.chat(chat_detail)
        tool_call_response = chat_response.data.chat_response.tool_calls

        tool_results = []

        # Iterate over the tool calls generated by the model
        if not tool_call_response:
            cohere_tool_results = CohereToolResult()
            cohere_tool_results.outputs = []
            chat_request.tool_results = cohere_tool_results.outputs
            chat_request.is_force_single_step = True
            chat_response = generative_ai_inference_client.chat(chat_detail)
            answer_items = chat_response.data.chat_response.text

        else:
            print("The model recommends doing the following tool calls:")
            print("\n".join(str(tool_call) for tool_call in chat_response.data.chat_response.tool_calls))
            for tool_call in tool_call_response:
                # here is where you would call the tool recommended by the model, using the parameters recommended by the model
                if tool_call.parameters is None:
                    tool_call.parameters = {}
                output = functions_map[tool_call.name](**tool_call.parameters)
                # store the output in a list
                outputs = output
                tool_result = CohereToolResult()
                tool_result.call = tool_call
                tool_result.outputs = outputs
                tool_results.append(tool_result)

            chat_request.tool_results = tool_results
            chat_request.is_force_single_step = True
            chat_detail.chat_request = chat_request
            chat_response = generative_ai_inference_client.chat(chat_detail)
            answer_items = chat_response.data.chat_response.text

        return answer_items

    st.chat_message("user").markdown(user_input)
    # Add user message to chat history
    st.session_state.chat_history.append({"role": "USER", "message": user_input})

    response = ai_response()
    # Display assistant response in chat message container
    with st.chat_message("CHATBOT"):
        st.markdown(response)
    # Add assistant response to chat history
    st.session_state.chat_history.append({"role": "CHATBOT", "message": response})